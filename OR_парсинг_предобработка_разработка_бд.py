# -*- coding: utf-8 -*-
"""OR_парсинг_предобработка_разработка_бд.ipynb""

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BEwByw3CTbelTv4V0LqdSAi4mEKwb5hO

# Исследование датасета опиоидных рецепторов

Загрузка и импорт необходимых библиотек и пакетов
"""

!pip install chembl_webresource_client rdkit pandas dash

# Импортируем нужные библиотеки
import pandas as pd
import numpy as np
import seaborn
import matplotlib.pyplot as plt
from scipy import stats as st
from rdkit import Chem
from rdkit.Chem import AllChem
import sqlite3
from chembl_webresource_client.new_client import new_client
from rdkit.DataStructs.cDataStructs import ConvertToNumpyArray
from rdkit.Chem import Descriptors, MolFromSmiles
import sqlite3
import dash
from dash import dcc, html
import plotly.express as px
from plotly.subplots import make_subplots
from matplotlib import pyplot as plt
import seaborn as sns
colors = ['#082040', '#175073', '#3285A6', '#B8D0D9', '#6CC5D9']

"""## 1. Загрузка датасетов
Опиоидных рецепторов пять, поэтому загрузим все датасеты, сольем их в один и добавим столбец, в котором будет содержаться информация из какого датасета иначально взяты данные.
Для данной задачи эффективнее использовать наработки из готовых БД, чем использовать парсинг данных. Используем бд ChEMBL для поиска данных.
"""

from google.colab import drive
drive.mount('/content/drive')

# Загрузим датафреймы
M_data = pd.read_csv('/content/drive/MyDrive/Opioid_receptor/M_opioid_IC50_Ki.csv', sep = ';', low_memory=False)
K_data = pd.read_csv('/content/drive/MyDrive/Opioid_receptor/K_opioid_IC50_Ki.csv', sep = ';', low_memory=False)
D_data = pd.read_csv('/content/drive/MyDrive/Opioid_receptor/D_opioid_IC50_Ki.csv', sep = ';', low_memory=False)
N_data = pd.read_csv('/content/drive/MyDrive/Opioid_receptor/N_opioid_IC50_Ki.csv', sep = ';', low_memory=False)

# Загрузим датафреймы с лигандами, соответствующими нескольким мишеням

M_data['Target'] = 'Mu'
K_data['Target'] = 'Kappa'
D_data['Target'] = 'Delta'
N_data['Target'] = 'Nociceptin'

data = pd.concat([M_data, K_data, D_data, N_data], axis=0, ignore_index=True)
data.head(2)

data.isna().sum()

# Общая информация о наборе данных, индекс, столбцы и тип данных, ненулевые значения и использование памяти
data.info()

data.describe(include = "all")

# Визуализируем общие данные
data.hist(figsize=(12, 10));

"""## 2. Предобработка данных

###2.1 Исследование значений и удаление нефункциональных колонок и строк

Удаляем колонки, не несущие информации о молекуле, которая нужна для прогноза структуры этой молекулы: 'Molecule Name', 'Molecule Max Phase', 'Molecule ChEMBL ID', 'Compound Key', 'Data Validity Comment', 'Comment', 'Uo Units', 'Ligand Efficiency BEI', 'Ligand Efficiency LE', 'Ligand Efficiency LLE','Ligand Efficiency SEI', 'Potential Duplicate', 'Assay ChEMBL ID', 'Assay Description', 'Assay Type', 'BAO Format ID', 'BAO Label', 'Assay Organism', 'Assay Tissue ChEMBL ID', 'Assay Tissue Name', 'Assay Cell Type', 'Assay Subcellular Fraction', 'Assay Parameters', 'Assay Variant Accession', 'Assay Variant Mutation', 'Target ChEMBL ID', 'Target Name', 'Target Organism',
'Target Type', 'Document ChEMBL ID', 'Source ID', 'Source Description', 'Document Journal', 'Document Year', 'Cell ChEMBL ID', 'Properties',
'Action Type', 'Standard Text Value'
"""

delete_column = ['Molecule Name', 'Molecule Max Phase', 'Molecule ChEMBL ID', 'Compound Key', 'Data Validity Comment', 'Comment', 'Uo Units',
                 'Ligand Efficiency BEI', 'Ligand Efficiency LE', 'Ligand Efficiency LLE','Ligand Efficiency SEI', 'Potential Duplicate',
                 'Assay ChEMBL ID', 'Assay Description', 'Assay Type', 'BAO Format ID', 'BAO Label', 'Assay Organism', 'Assay Tissue ChEMBL ID',
                 'Assay Tissue Name', 'Assay Cell Type', 'Assay Subcellular Fraction', 'Assay Parameters', 'Assay Variant Accession', 'Assay Variant Mutation',
                 'Target ChEMBL ID', 'Target Name', 'Target Organism', 'Target Type', 'Document ChEMBL ID', 'Source ID', 'Source Description',
                 'Document Journal', 'Document Year', 'Cell ChEMBL ID', 'Properties', 'Action Type', 'Standard Text Value']
data = data.drop(delete_column, axis=1)
data.describe(include = "all")

"""Удаление ошибочных smiles можно провести одновременно с получением Mol из Smiles, так как для сломанных smiles или ячеек с Nan Mol не будет получен. Далее можно удалить строки с Nan в колонке с Mol. Таким образом функция отработает быстрее, чем если бы мы проверяли smiles на валидность. Да и к тому же мы сразу получим еще один необходимый для дальнейших расчетов столбец."""

def is_valid_smiles(smiles):
    try:
        mol = Chem.MolFromSmiles(smiles)
        return mol
    except:
        return None

# Применение функции и фильтрация DataFrame
data['Mol'] = data['Smiles'].apply(is_valid_smiles)
df_cleaned = data.dropna(subset=['Mol']).reset_index(drop=True)
# Вывод результата
df_cleaned

# Общая информация о наборе данных, индекс, столбцы и тип данных, ненулевые значения и использование памяти
df_cleaned.info()

set_units = set(df_cleaned['Standard Units'])
set_units

set_type = set(df_cleaned['Standard Type'])
set_type

set_rel = set(df_cleaned['Standard Relation'])
set_rel

"""Удаление строк, для которых значения 'Standard Relation' не равны '='"""

index_to_drop = df_cleaned['Standard Relation'] != "'='"
df_cleaned.drop(index=index_to_drop.index[index_to_drop], inplace=True)
df_cleaned.info()

"""### 2.2 Разделение датафрейма на два по типу оценки биологической активности"""

df_ki = df_cleaned[df_cleaned['Standard Type'] == 'Ki']
df_ic50 = df_cleaned[df_cleaned['Standard Type'] == 'IC50']

df_ic50.info()

df_ic50.head(2)

df_ki.info()

"""Так как данных в датафрейме с Ki больше, то далее будем работать с ним

Общая предварительная оценка данных:
- набор данных достаточен для интеллектуального анализа
- есть пропуски, аномалии и выбросы
- присутствуют явные дубликаты, неправильные типы
- присутствууют структурные дубликаты с разными значениями Ki, скорее всего при парсинге данных в БД ChEMBL если значения Ki были представлены диапазоном - при парсинге границы диапазона были записаны в разные строки, - решается заменой двух строк на одну со средним значением из этого диапазона в ячейке с Ki.

### 2.3 Работа с дубликатами
"""

df_ki[df_ki.duplicated()].head(2)

#Удаление дубликатов
df_ki = df_ki.drop_duplicates()

df_ki.info()

set_rel = set(df_ki['Standard Relation'])
set_rel

# Группировка по всем столбцам, кроме ['Standard Value'] и ['pChEMBL Value']
grouped = df_ki.groupby(['Molecular Weight',	'#RO5 Violations',	'AlogP',	'Smiles',	'Standard Type',	'Standard Relation', 'Standard Units', 'Target'])

# Создание нового DataFrame с усреднёнными значениями
result = grouped.agg(
    Standard_Value=('Standard Value', 'mean'),
    pChEMBL_Value=('pChEMBL Value', 'mean'),  # Можно выбрать любое значение или усреднить
    Mol=('Mol', 'first')
).reset_index()

# Вывод результата
result

set(result['Smiles'])==set(df_ki['Smiles'])

set(result['Target'])==set(df_ki['Target'])

mask = df_ki['Smiles'].isin(result['Smiles'])

# Фильтрация строк
exclusive_df = df_ki[~mask]

# Вывод результата
exclusive_df

"""Так как для 345 молекул нет экспериментально полученных значений для #RO5 Violations и AlogP, и на данный момент у нас недостаточно данных, описывающих молекулу, то заполнять KNN не столь рационально. Мы можем при помощи библиотеки RDKit получить информацию об AlogP и правилах Липински, поэтому сейчас мы можем удалить эти столбцы из датафрейма  df_ki и заново провести работу с дубликатами."""

delete_column = ['#RO5 Violations',	'AlogP']
df_ki = df_ki.drop(delete_column, axis=1)
df_ki.describe(include = "all")

# Группировка по всем столбцам, кроме ['Standard Value'] и ['pChEMBL Value']
grouped = df_ki.groupby(['Molecular Weight',	'Smiles',	'Standard Type',	'Standard Relation', 'Standard Units', 'Target'])

# Создание нового DataFrame с усреднёнными значениями
df_ki_original = grouped.agg(
    Standard_Value=('Standard Value', 'mean'),
    pChEMBL_Value=('pChEMBL Value', 'mean'),  # Можно выбрать любое значение или усреднить
    Mol=('Mol', 'first')
).reset_index()

# Вывод результата
df_ki_original

set(df_ki_original['Smiles'])==set(df_ki['Smiles'])

mask_new = df_ki['Smiles'].isin(df_ki_original['Smiles'])

# Фильтрация строк
exclusive_df = df_ki[~mask_new]

# Вывод результата
exclusive_df.info()

"""### 2.4 Исследование и заполнение пропусков"""

# Выведем долю пропущенных значений для каждого столбца датафрейма
pd.DataFrame(df_ki_original.isna().mean()*100)

"""Есть много причин появления пропусков, например технические - ошибки системы, при передаче, выгрузке данных
В данном случае скорее всего не был проведен перерасчет pValue для Value, Но так как все значения Value известны, то мы можем привести перерасчет для pValue
"""

# Converting pIC50=-logIC50(μM) to nM
# Перевод Ki в pKi
df_ki_original['pChEMBL_Value'] = df_ki_original['Standard_Value'].apply(lambda x: -np.log10(x))

# Выведем долю пропущенных значений для каждого столбца датафрейма
pd.DataFrame(df_ki_original.isna().mean()*100)

df_ki_original.info()

# Визуализируем общие данные
df_ki_original.hist(figsize=(12, 10));

print("Итоги подготовки данных:")
print("-" * 40)
print("Всего строк изначально:", len(data))
print("Всего строк после подготовки:", len(df_ki_original))
print("Процент строк, которые были удалены:", ((len(data) - len(df_ki_original)) / len(data)) * 100, "%")
total_rows_initial = len(data)
total_rows_after = len(df_ki_original)
removed_percentage = ((total_rows_initial - total_rows_after) / total_rows_initial) * 100

"""### 2.5 Расчет физико-химических дескрипторов RDKit"""

df_ki_original = pd.read_csv('/content/drive/MyDrive/Opioid_receptor/df_ki_original.csv', sep = ',', low_memory=False)

def get_rdkit(df):
    computed_descriptors = Chem.Descriptors.descList
    for descriptor in computed_descriptors:
        name = descriptor[0]
        df[name] = df['Smiles'].apply(lambda x: descriptor[1](MolFromSmiles(x)))
    return df

df_ki_original = get_rdkit(df_ki_original)

df_ki_original.describe()

#Сохраним новый чистый датафрейм c дескрипторами
df_ki_original.to_csv('/content/drive/MyDrive/Opioid_receptor/df_ki_original_descriptors.csv', index=False)

"""## Создание БД"""

# Сохранение очищенной базы данных в SQLite
conn = sqlite3.connect('opioid_ligands.db')
df_ki_original.to_sql('opioid_ligands', conn, if_exists='replace', index=False)
conn.close()